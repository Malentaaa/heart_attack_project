{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предсказание риска сердечных приступов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом проекте предполагается работа с датасетом из открытого источника.Требуется разработать модель машинного обучения, а также подготовить библиотеку и интерфейс к ней для предсказания на тестовой выборке. \n",
    "В результате у вас должно получиться приложение на FastAPI (или аналогичном фреймворке) которое принимает на вход путь к csv файлу тестовой выборки, выполняет предсказание (например, путем POST запроса к сервису) и возвращает ответ в формате JSON. Допускается для тестирования написать скрипт, который посылает запрос к запущенному ранее приложению, либо реализовать веб-интерфейс.\n",
    "\n",
    "<b>Задачи проекта</b> \n",
    "\n",
    "Предполагается, что в ходе работы над проектом будут решены следующие задачи:\n",
    "●\tИсследование датасета (предполагается поиск дополнительной информации для лучшего понимания природы данных)\n",
    "●\tПредобработка данных. \n",
    "●\tОбучение модели\n",
    "●\tПодготовка предсказания на тестовой выборке. \n",
    "●\tПодготовка скриптов и библиотеки для обработки данных и предсказания на тестовой выборке\n",
    "●\tНаписание инструмента для тестирования\n",
    "●\tОформление документации\n",
    "\n",
    "<b>Ход исследования</b>   \n",
    "\n",
    "Данные пациентов для предсказания риска сердечных приступов\n",
    "-\tid  - id\n",
    "-\tАнтропометрические параметры (вес, возраст, рост)\n",
    "-\tПривычки (курение, качество сна и т.д)\n",
    "-\tДавление\n",
    "-\tНаличие хронических заболеваний\n",
    "-\tБиохимия крови\n",
    "-\tТаргет - высокий или низкий риск поражения сердца\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Загрузка и изучение данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn==1.3.2 -q "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install shap -q "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xgboost lightgbm catboost -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seaborn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install statsmodels -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import shap\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.stats import shapiro\n",
    "from pandas.api.types import is_integer_dtype\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, OrdinalEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import  (\n",
    "    fbeta_score, f1_score, precision_score, recall_score, roc_auc_score,\n",
    "    confusion_matrix, make_scorer, average_precision_score\n",
    ")\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import shap\n",
    "from typing import Optional, List\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.25\n",
    "TARGET_COL = \"heart_attack_risk_binary\"\n",
    "K_BEST       = 15                    \n",
    "SCORE_FUNC   = f_classif             \n",
    "sns.set() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_train = pd.read_csv('C:/Users/malen/OneDrive/Desktop/data_since/мастерская 1/data/raw/heart_train.csv', index_col='id')\n",
    "heart_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`heart_train` состоит из 8685 записей и 27 столбцов, 243 пропуска в отдельных столбцах. Требуется доп обработка данных object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_test = pd.read_csv('C:/Users/malen/OneDrive/Desktop/data_since/мастерская 1/data/raw/heart_test.csv', index_col='id')\n",
    "heart_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`heart_test` состоит из 8685 записей и 26 столбцов, 31 пропуск в отдельных столбцах. Требуется доп обработка данных object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_train['Unnamed: 0'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_test['Unnamed: 0'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный столбец использовался для нумерации строк в таблице, поэтому он нам не нужен."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_train = heart_train.drop('Unnamed: 0', axis=1)\n",
    "heart_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_test = heart_test.drop('Unnamed: 0', axis=1)\n",
    "heart_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполним предобработку данных. Изменим названия столбцов, типы данных, где это необходимо. Проведем проверку на явные и неявные дубликаты в данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Изменение наименования столбцов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приведем наименования столбцов к змеиному регистру с помощью регулярных выражений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_snake_case(name):\n",
    "    name = name.strip()\n",
    "    name = re.sub(r'[\\s\\-]+', '_', name)               # пробелы и дефисы → _\n",
    "    name = re.sub(r'([a-z0-9])([A-Z])', r'\\1_\\2', name) # camelCase → camel_case\n",
    "    name = re.sub(r'[^\\w_]', '', name)                  # убрать лишние символы\n",
    "    return name.lower()\n",
    "\n",
    "heart_train.columns = [to_snake_case(c) for c in heart_train.columns]\n",
    "heart_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_test.columns = [to_snake_case(c) for c in heart_test.columns]\n",
    "heart_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Обработка пропусков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_train[heart_train['stress_level'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В обучающем наборе данных пропуски встречаются только в 9 конкретных признаках:          \n",
    "`diabetes`, `family_history`, `smoking`, `obesity`, `alcohol_consumption`, `physical_activity_days_per_week`, `previous_heart_problems`, `medication_use`, `stress_level`.\n",
    "\n",
    "Все они связаны с образом жизни и историей болезни пациента.\n",
    "Количество пропусков одинаковое во всех этих колонках — 243 записи (≈ 2,8% датасета). Это значит, что пропуски не случайны, а возникают «пакетом» для целых строк, что, вероятно, связано с особенностями сбора данных (например, неполные анкеты или пациенты, которым не задавали эти вопросы).                \n",
    "Потеря 2,8% строк снизит размер обучающей выборки и может убрать целую подгруппу пациентов, что приведёт к смещению распределения признаков.\n",
    "\n",
    "В тестовых данных возможны такие же паттерны пропусков, и если мы их выбросим из train, модель не научится корректно обрабатывать такие случаи.\n",
    "Пропуски содержат потенциально полезную информацию — сам факт, что данные не заполнены, может коррелировать с вероятностью сердечного приступа (например, пациенты с неполной медицинской историей).\n",
    "\n",
    "Стратегия обработки NaN:\n",
    "- Сохранить все строки, включая те, где есть пропуски.\n",
    "- Импутировать пропуски. Для бинарных признаков (0/1) — мода по обучающим данным. Для порядковых признаков (`stress_level`, `physical_activity_days_per_week`) — медиана. Для остальных числовых признаков — медиана.\n",
    "\n",
    "Такой подход минимизирует потерю информации, сохраняет распределения признаков и при этом обеспечивает готовность данных для алгоритмов, которые не работают с NaN напрямую."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Обработка дубликатов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед построением модели прогнозирования риска сердечного приступа необходимо провести предварительную проверку качества исходных данных. Цель — выявить и устранить записи, которые могут исказить обучение модели: полные дубликаты строк, дубликаты с противоречивыми значениями целевого признака, а также неявные повторы.   \n",
    "Это позволяет снизить риск переобучения, исключить влияние повторяющихся пациентов и повысить обобщающую способность модели.\n",
    "\n",
    "Проверка на дубликаты выполняется в несколько этапов:\n",
    "1. Полные дубликаты\n",
    "- Ищем строки, полностью совпадающие по всем признакам (включая целевой признак).\n",
    "- Если такие строки есть, оставляем только первую, остальные удаляем.\n",
    "- Это исключает ситуации, когда один и тот же пациент учитывается несколько раз без изменений данных.\n",
    "\n",
    "2. Дубликаты по признакам с разными значениями целевого признака (конфликтные записи)\n",
    "- Определяем группы записей, полностью совпадающих по всем признакам, кроме целевого (`heart_attack_risk_binary`).\n",
    "- Если внутри группы встречаются разные значения целевого признака, удаляем все такие записи.\n",
    "- Это предотвращает попадание в обучающую выборку противоречивой информации, которая мешает алгоритму обучаться.\n",
    "\n",
    "3. Дубликаты по признакам с одинаковым целевым признаком (неявные дубликаты)\n",
    "- Если записи совпадают по всем признакам и целевому признаку, но различаются по индексу (ID), оставляем только первую.\n",
    "- Это снижает искусственное увеличение веса отдельных примеров в обучении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicates_and_conflicts(df: pd.DataFrame, target_col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Удаляет полные и конфликтные дубликаты и печатает статистику.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1) Полные дубликаты\n",
    "    full_dupes = df.duplicated(keep=\"first\")\n",
    "    n_full_dupes = full_dupes.sum()\n",
    "    df = df[~full_dupes]\n",
    "\n",
    "    # 2) Конфликтные дубликаты (одинаковые фичи, разный таргет)\n",
    "    feature_cols = [c for c in df.columns if c != target_col]\n",
    "    grp = df.groupby(feature_cols, dropna=False)[target_col].nunique()\n",
    "    conflict_keys = grp[grp > 1].index\n",
    "    n_conflict_groups = len(conflict_keys)\n",
    "\n",
    "    if n_conflict_groups > 0:\n",
    "        conflict_df = pd.DataFrame(list(conflict_keys), columns=feature_cols)\n",
    "        df = df.merge(conflict_df.assign(__conflict__=1), how=\"left\", on=feature_cols)\n",
    "        df = df[df[\"__conflict__\"].isna()].drop(columns=\"__conflict__\")\n",
    "\n",
    "    # 3) Неявные дубликаты (одинаковые фичи + одинаковый таргет)\n",
    "    feature_cols = [c for c in df.columns if c != target_col]\n",
    "    same_dupes = df.duplicated(subset=feature_cols + [target_col], keep=\"first\")\n",
    "    n_same_dupes = same_dupes.sum()\n",
    "    df = df[~same_dupes]\n",
    "\n",
    "    # --- Вывод результатов ---\n",
    "    print(\"Результат проверки для текущих данных\")\n",
    "    print(f\"Полные дубликаты: {n_full_dupes} записей.\")\n",
    "    print(f\"Дубликаты по признакам с разными целевыми значениями: {n_conflict_groups} групп.\")\n",
    "    print(f\"Неявные дубликаты (одинаковые признаки + одинаковый целевой): {n_same_dupes} записей.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_train = drop_duplicates_and_conflicts(heart_train, 'heart_attack_risk_binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, в данных нет явных или скрытых дубликатов, которые требовали бы удаления, однако проверка включена в предобработку для предотвращения подобных проблем при поступлении новых данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Нормализация столбца gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_train['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_test['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В исходных данных признак `gender` представлен в смешанном формате: часть записей — строковые значения (Male/Female), часть — числовые (0, 1, 0.0, 1.0). Такой разнородный тип данных может вызвать ошибки или некорректное поведение при автоматической обработке в пайплайне.\n",
    "Поэтому принято решение закодировать `gender` вручную в числовой бинарный формат (1 — мужчина, 0 — женщина) до подачи данных в модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_gender(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    male/female, '1'/'0', '1.0'/'0.0' -> {1.0, 0.0}, NaN сохраняем\n",
    "    \"\"\"\n",
    "    s = series.astype(str).str.strip().str.lower()\n",
    "    s = (s.replace({\"male\":\"1\",\"female\":\"0\"})\n",
    "           .str.replace(\".0\",\"\", regex=False)\n",
    "           .replace({\"nan\": np.nan}))\n",
    "    return pd.to_numeric(s, errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"gender\" in heart_train.columns:\n",
    "    heart_train[\"gender\"] = normalize_gender(heart_train[\"gender\"])\n",
    "if \"gender\" in heart_test.columns:\n",
    "    heart_test[\"gender\"] = normalize_gender(heart_test[\"gender\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_train['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_test['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, `gender` был преобразован в единый числовой флаг, что делает обработку данных более надёжной и предсказуемой для всех типов моделей, включая алгоритмы деревьев и линейные классификаторы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Изменение типа данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для бинарных, категориальных и порядковых признаков надёжнее переводить значения в int перед кодированием — это исключает неоднозначную интерпретацию и проблемы с обработкой, а так же уберет визуальный шум при построении графиков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_features = [\n",
    "    'diabetes',\n",
    "    'family_history',\n",
    "    'smoking',\n",
    "    'obesity',\n",
    "    'alcohol_consumption',\n",
    "    'previous_heart_problems',\n",
    "    'medication_use',\n",
    "    'gender',\n",
    "    \n",
    "    # Порядковые признаки\n",
    "    'diet',\n",
    "    'stress_level',\n",
    "    'physical_activity_days_per_week'\n",
    "]\n",
    "\n",
    "# Приведение к int\n",
    "for col in int_features:\n",
    "    heart_train[col] = heart_train[col].dropna().astype(int)\n",
    "    heart_test[col] = heart_test[col].dropna().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Большинство столбцов, требующих перевод в int, содержат NaN, поэтому смена типа лданных будет реализована после обработки NaN в пайплайне."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Исследовательский анализ данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Статистический анализ числовых признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Большинство признаков — числовые (float), но значения в диапазоне [0,1] встречаются очень часто, что говорит о масштабировании (min-max normalization) или кодировании как доли.\n",
    "Есть признаки, где значения — целые числа в небольших диапазонах.\n",
    "\n",
    "1. Признаки, вероятно, масштабированные в [0,1]\n",
    "- Почти все «медицинские» метрики имеют: min = 0, max = 1, Средние ≈ 0.45–0.50, std ~0.28\n",
    "- Это сильно похоже на min-max scaling до диапазона [0,1]. Примеры: `age`, `cholesterol`, `triglycerides`, `blood_sugar`, `ck_mb`, `troponin`.\n",
    "\n",
    "2. Бинарные признаки (0/1)\n",
    "- `diabetes`, `family_history`, `smoking`, `obesity`, `alcohol_consumption`, `previous_heart_problems`, `medication_use`, `gender`\n",
    "- Минимум = 0, максимум = 1, среднее в диапазоне 0.48–0.90 (разный баланс классов).\n",
    "- Вероятно, изначально были категориальные ответы («Да/Нет») → закодированы в 0/1.\n",
    "\n",
    "3. Порядковые признаки\n",
    "- `physical_activity_days_per_week`: min=0, max=7, медиана=3 → это кол-во активных дней в неделю.\n",
    "- `stress_level`: min=1, max=10, равномерное распределение по шкале.\n",
    "\n",
    "4. Целевой признак\n",
    "- `heart_attack_risk_binary`: баланс сильно смещён — среднее 0.3469 → ~34,7% положительных случаев.\n",
    "- Это несбалансированный датасет → при моделировании придётся учитывать (class_weight, oversampling/undersampling, metrics focus).\n",
    "\n",
    "Выводы:\n",
    "- Бинарные признаки — не масштабировать, просто импутировать модой.\n",
    "- Порядковые признаки — импутировать медианой.\n",
    "- Масштабированные признаки — оставить как есть, чтобы не «перемасштабировать».\n",
    "- Несбалансированность таргета — использовать метрики чувствительные к дисбалансу (ROC-AUC, PR-AUC, Recall) и балансировку классов в моделях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В тестовом наборе картинка почти та же, что и в train, что хорошо для обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед началом анализа признаков мы отдельно рассматриваем распределение целевого признака heart_attack_risk_binary. Этот шаг необходим, чтобы оценить баланс классов в выборке и понять, требуется ли в дальнейшем применение методов коррекции дисбаланса (например, использование параметра class_weight, oversampling или undersampling).\n",
    "\n",
    "В нашем случае целевой признак бинарный:\n",
    "- 0 — отсутствие риска сердечного приступа,\n",
    "- 1 — наличие риска.\n",
    "\n",
    "Анализ распределения позволит заранее определить долю каждого класса и выбрать подходящие метрики для оценки модели. При сильной несбалансированности классов особое внимание будет уделено метрикам, чувствительным к дисбалансу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Распределение целевого признака\n",
    "target_counts = heart_train[\"heart_attack_risk_binary\"].value_counts().sort_index()\n",
    "target_perc = target_counts / target_counts.sum() * 100\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.bar(target_counts.index.astype(str), target_counts.values, color=[\"skyblue\",\"salmon\"])\n",
    "plt.xticks([0,1], [\"0 — нет риска\", \"1 — риск\"])\n",
    "plt.ylabel(\"Количество записей\")\n",
    "plt.title(\"Распределение целевого признака\")\n",
    "\n",
    "# Печать чисел и процентов\n",
    "for idx, val in enumerate(target_counts.values):\n",
    "    plt.text(idx, val + 10, f\"{val} ({target_perc.iloc[idx]:.1f}%)\", ha=\"center\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(target_counts)\n",
    "print(target_perc.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После изучения распределения целевого признака переходим к анализу всех входных признаков в обучающем наборе данных. Цель этого этапа — выявить особенности распределений, определить тип каждого признака (бинарный, дискретный, порядковый или непрерывный), а также зафиксировать потенциальные аномалии или выбросы.\n",
    "\n",
    "Для каждого числового признака строятся:\n",
    "- Диаграмма распределения (гистограмма для непрерывных признаков или столбчатая диаграмма для дискретных/бинарных);\n",
    "- Боксплот для визуального обнаружения выбросов и оценки диапазона значений.\n",
    "\n",
    "Этот анализ помогает:\n",
    "- Определить признаки, требующие особой предобработки (масштабирование, кодирование, удаление выбросов);\n",
    "- Проверить согласованность данных в train и test;\n",
    "- Найти признаки, которые уже нормированы или закодированы, чтобы избежать лишней обработки в пайплайне.\n",
    "\n",
    "Особое внимание уделяется бинарным и порядковым признакам, так как для них используются отдельные стратегии импутации и обработки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COL = \"heart_attack_risk_binary\"\n",
    "\n",
    "# 1) эвристика дискретности (как и раньше)\n",
    "def is_binary_feature(s: pd.Series) -> bool:\n",
    "    vals = set(pd.to_numeric(s.dropna(), errors=\"coerce\").unique().tolist())\n",
    "    return len(vals) <= 2 and vals.issubset({0, 1})\n",
    "\n",
    "def is_discrete_feature(s: pd.Series, threshold: int = 15) -> bool:\n",
    "    if is_binary_feature(s):\n",
    "        return True\n",
    "    nunique_ok = s.nunique(dropna=True) < threshold\n",
    "    if is_integer_dtype(s):\n",
    "        return nunique_ok\n",
    "    s_num = pd.to_numeric(s.dropna(), errors=\"coerce\")\n",
    "    all_int_like = (s_num == np.floor(s_num)).all()\n",
    "    return nunique_ok and bool(all_int_like)\n",
    "\n",
    "# 2) красивые подписи (дополни по вкусу)\n",
    "axis_labels = {\n",
    "    \"age\": \"Возраст (нормировано)\",\n",
    "    \"cholesterol\": \"Холестерин (нормировано)\",\n",
    "    \"heart_rate\": \"ЧСС (нормировано)\",\n",
    "    \"diabetes\": \"Сахарный диабет (0/1)\",\n",
    "    \"family_history\": \"Семейный анамнез (0/1)\",\n",
    "    \"smoking\": \"Курение (0/1)\",\n",
    "    \"obesity\": \"Ожирение (0/1)\",\n",
    "    \"alcohol_consumption\": \"Потребление алкоголя (0/1)\",\n",
    "    \"exercise_hours_per_week\": \"Часы активности/нед (нормировано)\",\n",
    "    \"diet\": \"Диета (0..3)\",\n",
    "    \"previous_heart_problems\": \"Проблемы с сердцем ранее (0/1)\",\n",
    "    \"medication_use\": \"Приём лекарств (0/1)\",\n",
    "    \"stress_level\": \"Уровень стресса (1..10)\",\n",
    "    \"sedentary_hours_per_day\": \"Сидячие часы/день (нормировано)\",\n",
    "    \"income\": \"Доход (нормировано)\",\n",
    "    \"bmi\": \"ИМТ (нормировано)\",\n",
    "    \"triglycerides\": \"Триглицериды (нормировано)\",\n",
    "    \"physical_activity_days_per_week\": \"Дни активности/нед (0..7)\",\n",
    "    \"sleep_hours_per_day\": \"Сон/день (нормировано)\",\n",
    "    \"blood_sugar\": \"Глюкоза (нормировано)\",\n",
    "    \"ck_mb\": \"CK-MB (нормировано)\",\n",
    "    \"troponin\": \"Тропонин (нормировано)\",\n",
    "    \"gender\": \"Пол (0=F,1=M)\",\n",
    "    \"systolic_blood_pressure\": \"Систолическое АД (нормировано)\",\n",
    "    \"diastolic_blood_pressure\": \"Диастолическое АД (нормировано)\",\n",
    "}\n",
    "\n",
    "# 3) удобная палитра «по числу столбиков»\n",
    "def palette_for_k(k: int):\n",
    "    # tab20 до 20, дальше — husl\n",
    "    if k <= 12:\n",
    "        return sns.color_palette(\"Set3\", k)\n",
    "    elif k <= 20:\n",
    "        return sns.color_palette(\"tab20\", k)\n",
    "    else:\n",
    "        return sns.color_palette(\"husl\", k)\n",
    "\n",
    "# 4) построение графиков\n",
    "def plot_feature_distributions(df: pd.DataFrame, dataset_name: str = \"train\", with_hue: bool = False):\n",
    "    num_cols = df.select_dtypes(include=\"number\").columns.tolist()\n",
    "    # исключаем id/target (если вдруг присутствуют)\n",
    "    num_cols = [c for c in num_cols if c.lower() not in (\"id\",)]\n",
    "    if TARGET_COL in num_cols:\n",
    "        num_cols.remove(TARGET_COL)\n",
    "\n",
    "    for col in num_cols:\n",
    "        s = df[col]\n",
    "        label = axis_labels.get(col, col)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "        if is_discrete_feature(s, threshold=15):\n",
    "            # порядок категорий (отсортированный)\n",
    "            cats = sorted(s.dropna().unique().tolist())\n",
    "            pal = palette_for_k(len(cats))\n",
    "\n",
    "            if with_hue and TARGET_COL in df.columns:\n",
    "                # Раскраска по таргету (столбики внутри каждой категории)\n",
    "                sns.countplot(\n",
    "                    data=df, x=col, hue=TARGET_COL,\n",
    "                    order=cats, palette=\"Set2\", ax=axes[0]\n",
    "                )\n",
    "                axes[0].legend(title=\"target\", loc=\"best\")\n",
    "            else:\n",
    "                # Каждый столбик своим цветом\n",
    "                sns.countplot(\n",
    "                    data=df, x=col,\n",
    "                    order=cats, palette=pal, ax=axes[0]\n",
    "                )\n",
    "\n",
    "            axes[0].set_title(f\"Распределение (дискретно):\\n{label} [{dataset_name}]\")\n",
    "            axes[0].set_xlabel(label)\n",
    "            axes[0].set_ylabel(\"Количество записей\")\n",
    "        else:\n",
    "            # непрерывное — обычная гистограмма\n",
    "            sns.histplot(s.dropna(), bins=20, kde=False, ax=axes[0], color=\"#4C72B0\")\n",
    "            axes[0].set_title(f\"Распределение:\\n{label} [{dataset_name}]\")\n",
    "            axes[0].set_xlabel(label)\n",
    "            axes[0].set_ylabel(\"Количество записей\")\n",
    "\n",
    "        # Боксплот (единый)\n",
    "        sns.boxplot(x=s, ax=axes[1], color=\"#88CCEE\")\n",
    "        axes[1].set_title(f\"Боксплот:\\n{label} [{dataset_name}]\")\n",
    "        axes[1].set_xlabel(label)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# --- запуск для train ---\n",
    "plot_feature_distributions(heart_train, dataset_name=\"train\", with_hue=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Бинарные признаки\n",
    "Примеры: `gender`, `diabetes`, `family_history`, `smoking`, `obesity`, `alcohol_consumption` и др.\n",
    "\n",
    "Распределение:\n",
    "`gender` — доля мужчин значительно выше (~72%), что создаёт дисбаланс.\n",
    "У некоторых других бинарных фичей (например, smoking) также наблюдается перекос в сторону одного класса.\n",
    "Пропуски: Визуально на countplot не видны, но ранее зафиксированы групповые NaN в одних и тех же записях.\n",
    "\n",
    "2. Порядковые признаки\n",
    "Примеры: `stress_level`, `diet`, `physical_activity_days_per_week`.\n",
    "\n",
    "Распределение:\n",
    "`physical_activity_days_per_week` — заметен пик на нуле (полное отсутствие активности).\n",
    "`diet` — редкие высокие значения (3), которые могут быть как выбросами, так и признаком малой подгруппы.\n",
    "\n",
    "3. Непрерывные признаки\n",
    "Примеры: `age`, `cholesterol`, `heart_rate`, `bmi`, `triglycerides`, `systolic_bp`, `diastolic_bp`, `glucose`, `ck_mb`, `troponin`.\n",
    "\n",
    "Распределения:\n",
    "`age`— смещение в сторону старших пациентов.\n",
    "`systolic_bp` и `diastolic_bp` — равномерные распределения, выбросов почти нет.\n",
    "`cholesterol` — умеренно симметричен, редкие выбросы.\n",
    "`glucose` — скошен вправо, есть значимые выбросы в верхнем диапазоне.\n",
    "`ck_mb` и `troponin` — очень сильный правый перекос, множество выбросов, >95% значений близки к нулю."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distributions(heart_test, dataset_name=\"test\", with_hue=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Бинарные признаки\n",
    "`gender`:\n",
    "- В обоих наборах мужчин заметно больше, чем женщин.\n",
    "- Доля мужчин в test немного выше, чем в train, но разница не критична.\n",
    "- другие бинарные признаки (например, smoking, diabetes, если присутствуют в тесте):\n",
    "- Общий тренд похож — классы дисбалансированы, но структура дисбаланса сохраняется.\n",
    "\n",
    "📌 Вывод: распределения бинарных признаков между train и test схожи, модель не должна испытывать проблем из-за этого.\n",
    "\n",
    "2. Непрерывные признаки (уже нормированы)\n",
    "Систолическое АД:\n",
    "- Train и test имеют почти равномерное распределение от 0.2 до 0.8, без значительных выбросов.\n",
    "\n",
    "Диастолическое АД:\n",
    "- Сходная картина в обеих выборках, разброс и медианы совпадают.\n",
    "\n",
    "📌 Вывод: существенного сдвига распределения нет.\n",
    "\n",
    "3. Биомаркеры (CK-MB, Troponin)\n",
    "- В обоих наборах распределения крайне скошены влево (основная масса значений около 0).\n",
    "- Количество выбросов в test чуть меньше, чем в train, что может говорить о меньшей доле пациентов в острой фазе заболевания.\n",
    "\n",
    "📌 Вывод: распределения совпадают по форме, но частота высоких значений в test чуть ниже. Это может немного снизить чувствительность модели на тесте к выявлению тяжёлых случаев.\n",
    "\n",
    "4. Признаки образа жизни\n",
    "`exercise_hours_per_week`:\n",
    "- В train распределение более \"сосредоточено\" на среднем уровне активности, в test чуть более равномерно.\n",
    "\n",
    "`sleep_hours_per_day`:\n",
    "- Почти одинаковые распределения в обеих выборках.\n",
    "\n",
    "`sedentary_hours_per_day`:\n",
    "- В test немного больше значений в верхней части диапазона, что может говорить о более малоподвижной группе.\n",
    "\n",
    "📌 Вывод: различия есть, но незначительные — вероятно, модель не потребует дополнительной адаптации.\n",
    "\n",
    "5. Пропуски\n",
    "- И в train, и в test пропуски встречаются группами (одни и те же признаки пустые в одной записи).\n",
    "- Вероятно, это пациенты, у которых не проводились отдельные обследования.\n",
    "\n",
    "📌 Вывод: выбранная для train стратегия импутации пропусков должна использоваться и для test, чтобы избежать смещения распределений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Выводы исследовательского анализа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные чистые, без дубликатов, с системными пропусками, которые можно обрабатывать предсказуемо.            \n",
    "Распределения признаков между train и test в целом совпадают, что снижает риск снижения качества модели при переходе на тестовые данные.            \n",
    "Основные риски — дисбаланс некоторых бинарных признаков и сильная скошенность биомаркеров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Корреляционный анализ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Матрица корреляции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_matrix(df, dataset_name=\"train\"):\n",
    "    # только числовые признаки\n",
    "    num_df = df.select_dtypes(include='number')\n",
    "    corr = num_df.corr()\n",
    "\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    sns.heatmap(\n",
    "        corr,\n",
    "        cmap='coolwarm',\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        annot_kws={\"size\": 8},\n",
    "        cbar_kws={'shrink': 0.8}\n",
    "    )\n",
    "    plt.title(f\"Корреляционная матрица ({dataset_name})\", fontsize=14, pad=12)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_correlation_matrix(heart_train, dataset_name=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Общая картина\n",
    "- Матрица корреляций показывает в целом низкие связи между признаками: большинство коэффициентов Пирсона находятся в диапазоне -0.05…0.05.\n",
    "- Ярко выраженных мультиколлинеарных пар (>|0.8|) нет — это значит, что модель не будет страдать от сильной избыточности признаков.\n",
    "\n",
    "2. Выделяющиеся связи\n",
    "- `smoking` ↔ `age`: -0.40 — отрицательная корреляция, указывает, что курение чаще встречается у более молодых пациентов.\n",
    "- `gender` ↔ `smoking`: 0.50 — сильная положительная корреляция, вероятно, курение больше распространено среди одного пола.\n",
    "- `age` ↔ `smoking`: уже упомянутая обратная связь, что подтверждает влияние возраста на привычки.\n",
    "- Остальные связи не превышают |0.05|, что говорит о слабой линейной зависимости между большинством признаков.\n",
    "\n",
    "3. Целевой признак\n",
    "- `heart_attack_risk_binary` слабо коррелирует с любыми отдельными признаками (|corr| < 0.05). Это ожидаемо для медицинских данных, где предсказание требует комбинации факторов.\n",
    "- Это значит, что модель должна улавливать нелинейные и многомерные зависимости, а не полагаться на один-два показателя.\n",
    "\n",
    "4. Выводы для моделирования\n",
    "- Опасности мультиколлинеарности нет — можно оставлять все признаки, кроме, возможно, пары smoking и gender, где корреляция 0.50 может дать избыточность в линейных моделях.\n",
    "- Слабые связи с таргетом — признак того, что линейная регрессия в чистом виде будет слабой, а деревья решений, бустинг или нейросети смогут извлечь больше пользы из комбинаций признаков.\n",
    "\n",
    "При отборе признаков лучше опираться не только на корреляцию, но и на feature importance после обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Анализ взаимосвязий целевого признака с входными признаками"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нашем случае потенциально подозрительными могут быть:\n",
    "\n",
    "1. `previous_heart_problems`\n",
    "- Логично, что прошлые проблемы с сердцем напрямую связаны с риском сердечного приступа.\n",
    "- Если этот признак фиксируется только для тех, у кого уже был диагноз, он может быть слишком сильным предиктором и отражать уже случившееся событие, а не риск.\n",
    "\n",
    "2. `medication_use`\n",
    "- Если в данные попадают лекарства, назначенные после выявления болезни, это прямой путь к утечке.\n",
    "- Особенно, если эти препараты применяются исключительно при лечении сердечных заболеваний.\n",
    "- Но у нас нет прямых данных подтверждающих это \n",
    "\n",
    "3. `ck_mb` и `troponin`\n",
    "- Это биомаркеры, которые часто измеряют во время или сразу после инфаркта.\n",
    "- Их значения могут не просто коррелировать, а фактически указывать, что событие уже произошло.\n",
    "- Для задач прогнозирования такие признаки обычно исключают или используют только в задачах пост-фактум диагностики.\n",
    "\n",
    "4. `blood_sugar`, `cholesterol`, `triglycerides` (в меньшей степени)\n",
    "- Сами по себе не являются утечкой, но если замеры проводились после острого эпизода, они могут содержать следы уже случившегося события.\n",
    "- К сожалению более точных данных у нас нет\n",
    "\n",
    "5. `medication_use` + `previous_heart_problems` в связке\n",
    "- Если модель увидит и то, и другое, она может почти напрямую \"угадывать\" класс.\n",
    "\n",
    "Таким образом, признаки `ck_mb` и `troponin` подлежат удалению. По остальным надо смотреть SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед построением и отбором моделей важно понять, какие входные признаки могут быть информативны для прогнозирования целевой переменной `heart_attack_risk_binary` \n",
    "Для этого мы разделим признаки на два типа:\n",
    "- Бинарные и дискретные — визуализируем через countplot, чтобы сравнить распределения значений между классами таргета.\n",
    "- Непрерывные числовые — визуализируем через boxplot (по нормированным данным), чтобы увидеть различия в медианах и выбросах между классами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = heart_train.select_dtypes(include=\"number\").columns.tolist()\n",
    "feature_cols = [c for c in num_cols if c != TARGET_COL]\n",
    "\n",
    "for i in range(0, len(feature_cols), 2):\n",
    "    n_subplots = min(2, len(feature_cols) - i)\n",
    "    fig, axes = plt.subplots(1, n_subplots, figsize=(14, 5))\n",
    "    if n_subplots == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for j in range(n_subplots):\n",
    "        col = feature_cols[i + j]\n",
    "        label = axis_labels.get(col, col)\n",
    "\n",
    "        # данные для текущего признака без NaN\n",
    "        dfp = heart_train[[TARGET_COL, col]].dropna()\n",
    "\n",
    "        # если пусто — пропускаем\n",
    "        if dfp.empty:\n",
    "            axes[j].set_visible(False)\n",
    "            continue\n",
    "\n",
    "        # выбор типа графика\n",
    "        if is_discrete_feature(heart_train[col]):\n",
    "            # --- дискретный/бинарный: countplot\n",
    "            order_x = sorted(dfp[col].unique())\n",
    "            hue_order = sorted(dfp[TARGET_COL].unique())\n",
    "            sns.countplot(\n",
    "                data=dfp, x=col, hue=TARGET_COL,\n",
    "                order=order_x, hue_order=hue_order, ax=axes[j]\n",
    "            )\n",
    "            axes[j].set_title(f'Распределение (countplot): {label} по классам таргета')\n",
    "            axes[j].set_xlabel(label)\n",
    "            axes[j].set_ylabel('Количество')\n",
    "            axes[j].legend(title='Класс (0/1)')\n",
    "        else:\n",
    "            # --- непрерывный (нормированный): boxplot\n",
    "            # если остался только один класс — смысла в boxplot нет\n",
    "            cls_present = sorted(dfp[TARGET_COL].unique())\n",
    "            if len(cls_present) < 2:\n",
    "                axes[j].text(0.5, 0.5, 'Недостаточно классов для boxplot',\n",
    "                             ha='center', va='center', transform=axes[j].transAxes)\n",
    "                axes[j].set_axis_off()\n",
    "                continue\n",
    "\n",
    "            sns.boxplot(\n",
    "                data=dfp, x=TARGET_COL, y=col,\n",
    "                order=cls_present, showfliers=True, ax=axes[j]\n",
    "            )\n",
    "            axes[j].set_title(f'Распределение (boxplot): {label} по классам таргета')\n",
    "            axes[j].set_xlabel('Класс (0/1)')\n",
    "            axes[j].set_ylabel(label)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Анализ boxplot и countplot показал, что распределение ряда признаков заметно различается между классами таргета, что говорит о потенциальной информативности для модели.\n",
    "Особенно выраженные различия наблюдаются по признакам:\n",
    "- возраст, уровень холестерина, ЧСС, систолическое и диастолическое давление, ИМТ;\n",
    "- наличие сахарного диабета, семейного анамнеза, курение, ожирение, потребление алкоголя, проблемы с сердцем ранее, приём лекарств;\n",
    "- уровень стресса, часы и дни активности, сидячие часы, доход.\n",
    "\n",
    "Вместе с тем, признаки CK_MB и тропонин исключаем из дальнейшего моделирования, так как это биомаркеры, которые часто измеряют во время или сразу после инфаркта. Их наличие в данных может привести к утечке информации о целевой переменной, делая модель нереалистично точной, но плохо применимой на практике.\n",
    "\n",
    "Остальные признаки сохраняем для построения модели, так как они представляют ценность для предсказания и не содержат прямой утечки информации о таргете."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Портрет потенциального претендента на сердечный приступ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот обобщённый портрет потенциального претендента на сердечный приступ на основе анализа твоих графиков и данных проекта (без учёта постинфарктных биомаркеров CK-MB и тропонин):\n",
    "\n",
    "1. 👤 Социально-демографический профиль\n",
    "- Пол: чаще мужчины\n",
    "- Возраст: преимущественно старше среднего по выборке\n",
    "- Доход: умеренный или низкий (чёткой связи с высоким доходом нет)\n",
    "\n",
    "2. 🩺 Медицинские факторы\n",
    "- Часто присутствует артериальная гипертензия (систолическое и диастолическое давление выше среднего)\n",
    "- Повышенный уровень холестерина и триглицеридов\n",
    "- Наличие сахарного диабета\n",
    "- Лишний вес или ожирение (высокий ИМТ)\n",
    "- Наличие проблем с сердцем в прошлом\n",
    "- Приём лекарств (вероятно, для контроля давления, холестерина, сахара)\n",
    "\n",
    "3. 💡 Поведенческие факторы\n",
    "- Курение\n",
    "- Умеренное или высокое потребление алкоголя\n",
    "- Низкая физическая активность (меньше дней и часов активности в неделю)\n",
    "- Преобладание сидячего образа жизни (много сидячих часов в день)\n",
    "- Несбалансированная диета (низкие оценки по шкале качества питания)\n",
    "\n",
    "4. ⚡ Психоэмоциональные факторы\n",
    "- Часто высокий уровень стресса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/my_image.png\" alt=\"Описание\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Выводы по корреляционному анализу"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сильных линейных зависимостей почти нет — большинство коэффициентов Пирсона между признаками < 0.05.\n",
    "Умеренная корреляция наблюдается между:\n",
    "- `age` и `smoking` (≈ -0.40) — старшие респонденты реже курят.\n",
    "- `smoking` и `gender` (≈ 0.50) — в данных мужчины курят чаще.\n",
    "\n",
    "Медицинские показатели между собой почти не связаны — например, холестерин, давление, триглицериды и глюкоза не демонстрируют выраженной линейной зависимости.\n",
    "Явных признаков утечки на основании только корреляций не обнаружено, но медико-диагностические признаки (например, troponin, ck_mb) указывают на явную утечку через логику задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Построение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед обучением модели необходимо провести предобработку, чтобы привести данные к единому формату, корректно обработать пропуски и закодировать категориальные признаки.\n",
    "В данном блоке создаётся единый конвейер подготовки данных (preprocessor), который\n",
    "1. Обрабатывает пропуски:\n",
    "- Для числовых и порядковых признаков используется собственный класс GroupMedianImputer (импутация медианой).\n",
    "- Для бинарных и категориальных признаков применяется ModeImputer (импутация модой).\n",
    "\n",
    "2. Кодирует признаки:\n",
    "- Бинарные признаки очищаются и приводятся к {0, 1} с помощью BinaryCleaner.\n",
    "- Для категорий, где порядок заранее известен, используется OrdinalEncoder.\n",
    "\n",
    "Такой подход позволяет в одном объекте ColumnTransformer объединить обработку разных типов данных и применять её единообразно как к обучающей, так и к тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Удаляем признаки-утечки прямо из DataFrame: CK-MB и Troponin ==============\n",
    "heart_train = heart_train.drop(columns=[\"ck_mb\", \"troponin\"], errors=\"ignore\")\n",
    "\n",
    "# ---------------- кастомные трансформеры ---------------- #\n",
    "class GroupMedianImputer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        X_df = pd.DataFrame(X)\n",
    "        self.medians_ = X_df.median(numeric_only=False)\n",
    "        self.feature_names_in_ = X_df.columns.to_list()\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return pd.DataFrame(X, columns=self.feature_names_in_).fillna(self.medians_).values\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array(self.feature_names_in_ if input_features is None else input_features)\n",
    "\n",
    "class ModeImputer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        X_df = pd.DataFrame(X)\n",
    "        self.modes_ = X_df.mode(dropna=True).iloc[0]\n",
    "        self.feature_names_in_ = X_df.columns.to_list()\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return pd.DataFrame(X, columns=self.feature_names_in_).fillna(self.modes_).values\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array(self.feature_names_in_ if input_features is None else input_features)\n",
    "\n",
    "class BinaryCleaner(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        self.feature_names_in_ = pd.DataFrame(X).columns.to_list()\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        arr = np.asarray(X, dtype=float)\n",
    "        arr = np.rint(arr)\n",
    "        arr = np.clip(arr, 0, 1)\n",
    "        return arr.astype(np.int8)\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array(self.feature_names_in_ if input_features is None else input_features)\n",
    "\n",
    "class MissingIndicatorSimple(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        self.feature_names_in_ = pd.DataFrame(X).columns.to_list()\n",
    "        self.out_names_ = [f\"{c}__was_missing\" for c in self.feature_names_in_]\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_df = pd.DataFrame(X, columns=self.feature_names_in_)\n",
    "        return X_df.isna().astype(np.int8).values\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array(self.out_names_)\n",
    "\n",
    "# ---------------- списки признаков под наш проект ---------------- #\n",
    "# NB: ck_mb и troponin уже удалены из самого df; формируем фичи без них\n",
    "all_cols = [c for c in heart_train.columns if c != TARGET_COL]\n",
    "\n",
    "binary_features = [\n",
    "    \"diabetes\",\"family_history\",\"smoking\",\"obesity\",\"alcohol_consumption\",\n",
    "    \"previous_heart_problems\",\"medication_use\",\"gender\"\n",
    "]\n",
    "binary_features = [c for c in binary_features if c in all_cols]\n",
    "\n",
    "ordinal_features = [\n",
    "    \"diet\", \"stress_level\", \"physical_activity_days_per_week\"\n",
    "]\n",
    "ordinal_features = [c for c in ordinal_features if c in all_cols]\n",
    "\n",
    "numeric_features = [c for c in all_cols if c not in binary_features + ordinal_features]\n",
    "\n",
    "# ---------------- пайплайны по типам ---------------- #\n",
    "num_pipe = Pipeline([\n",
    "    (\"imp\", GroupMedianImputer()),\n",
    "    # (\"scaler\", StandardScaler()),\n",
    "])\n",
    "\n",
    "ord_pipe = Pipeline([\n",
    "    (\"imp\", GroupMedianImputer())\n",
    "])\n",
    "\n",
    "bin_pipe = Pipeline([\n",
    "    (\"imp\", ModeImputer()),\n",
    "    (\"bin\", BinaryCleaner())\n",
    "])\n",
    "\n",
    "# ---------------- единый препроцессор ---------------- #\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_pipe, numeric_features),\n",
    "        (\"ord\", ord_pipe, ordinal_features),\n",
    "        (\"bin\", bin_pipe, binary_features),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Обучение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим несколько моделей. Проверим качество лучшей модели с помощью Метрики ROC-AUC.     \n",
    "Выбор ROC-AUC как основной метрики не случаен, потому что она показывает разделяющую способность модели на всём диапазоне порогов и не зависит от того, где именно мы установливаем границу классификации.         \n",
    "\n",
    "Для медицинской задачи это важно: нам нужно сначала понять, насколько хорошо модель в принципе отличает здоровых от больных. А когда мы выберем лучшие модели по ROC-AUC, дальше уже будем подбирать порог именно под задачу — здесь для меня ключевой будет метрика F2, так как она сильнее учитывает Recall, а значит позволяет минимизировать риск пропустить больного пациента, что критично в медицине."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Данные для классификации\n",
    "X_cls = heart_train.drop(columns=[TARGET_COL]).copy()\n",
    "y_cls = heart_train[TARGET_COL].astype(int).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——— конфиг моделей с параметрами ------------------------------------------------\n",
    "models_cfg = {\n",
    "    'Random Forest': {\n",
    "        'est': RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
    "        'params': {\n",
    "            'clf__n_estimators': [100, 200, 300],\n",
    "            'clf__max_depth': [None, 10, 20],\n",
    "            'clf__min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'est': XGBClassifier(random_state=42, eval_metric='logloss', use_label_encoder=False),\n",
    "        'params': {\n",
    "            'clf__n_estimators': [100, 200],\n",
    "            'clf__learning_rate': [0.01, 0.1],\n",
    "            'clf__max_depth': [3, 5, 7]\n",
    "        }\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'est': LGBMClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'clf__n_estimators': [100, 200],\n",
    "            'clf__learning_rate': [0.01, 0.1],\n",
    "            'clf__max_depth': [-1, 10, 20]\n",
    "        }\n",
    "    },\n",
    "    'CatBoost': {\n",
    "        'est': CatBoostClassifier(random_state=42, verbose=0),\n",
    "        'params': {\n",
    "            'clf__iterations': [200, 500],\n",
    "            'clf__depth': [4, 6, 8],\n",
    "            'clf__learning_rate': [0.01, 0.1]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- внешний hold-out (единый для всех моделей) ---\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_cls, y_cls, test_size=0.2, stratify=y_cls, random_state=42\n",
    ")\n",
    "\n",
    "# ——— обучение и поиск лучших моделей ------------------------------------------------\n",
    "trained_cls   = {}      # name -> best_estimator_\n",
    "scores_cv_auc = {}      # name -> best_cv_auc\n",
    "scores_tr_auc = {}      # name -> train_auc (на X_tr)\n",
    "scores_ho_auc = {}      # name -> holdout_auc (на X_val)\n",
    "all_results   = []      # список DataFrame(cv_results_) по моделям\n",
    "\n",
    "start_all = time.time()\n",
    "\n",
    "for name, cfg in models_cfg.items():\n",
    "    print(f\"🔹 Обучаем {name}...\")\n",
    "    start_model = time.time()\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        ('prep', preprocessor),\n",
    "        ('clf',  cfg['est'])\n",
    "    ])\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        estimator=pipe,\n",
    "        param_grid=cfg['params'],\n",
    "        cv=5,\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "        refit=True\n",
    "    )\n",
    "\n",
    "    # подбираем гиперпараметры только на X_tr, y_tr\n",
    "    grid.fit(X_tr, y_tr)\n",
    "\n",
    "    # лучшая модель по CV\n",
    "    best_est = grid.best_estimator_\n",
    "    trained_cls[name]   = best_est\n",
    "    scores_cv_auc[name] = grid.best_score_\n",
    "\n",
    "    # --- ROC-AUC на трейне (X_tr) ---\n",
    "    if hasattr(best_est.named_steps['clf'], \"predict_proba\"):\n",
    "        tr_proba = best_est.predict_proba(X_tr)[:, 1]\n",
    "        ho_proba = best_est.predict_proba(X_val)[:, 1]\n",
    "    else:\n",
    "        # fallback для моделей без predict_proba\n",
    "        tr_dec = best_est.decision_function(X_tr)\n",
    "        ho_dec = best_est.decision_function(X_val)\n",
    "        tr_proba = (pd.Series(tr_dec).rank(method=\"average\") / len(tr_dec)).values\n",
    "        ho_proba = (pd.Series(ho_dec).rank(method=\"average\") / len(ho_dec)).values\n",
    "\n",
    "    tr_auc = roc_auc_score(y_tr, tr_proba)\n",
    "    ho_auc = roc_auc_score(y_val, ho_proba)\n",
    "    scores_tr_auc[name] = tr_auc\n",
    "    scores_ho_auc[name] = ho_auc\n",
    "\n",
    "    # сохраняем полные cv_results_\n",
    "    df_results = pd.DataFrame(grid.cv_results_)\n",
    "    df_results[\"model\"] = name\n",
    "    all_results.append(df_results)\n",
    "\n",
    "    elapsed_model = time.time() - start_model\n",
    "    print(f\"✅ {name}: CV ROC-AUC = {grid.best_score_:.4f} | \"\n",
    "          f\"Train ROC-AUC = {tr_auc:.4f} | Hold-out ROC-AUC = {ho_auc:.4f} | \"\n",
    "          f\"Параметры: {grid.best_params_}\")\n",
    "    print(f\"⏱ Время обучения {name}: {elapsed_model:.2f} секунд\\n\")\n",
    "\n",
    "# сводная таблица по моделям\n",
    "summary = (pd.DataFrame({\n",
    "    \"model\": list(scores_cv_auc.keys()),\n",
    "    \"roc_auc_cv\":     [scores_cv_auc[m] for m in scores_cv_auc],\n",
    "    \"roc_auc_train\":  [scores_tr_auc[m] for m in scores_tr_auc],\n",
    "    \"roc_auc_holdout\":[scores_ho_auc[m] for m in scores_ho_auc],\n",
    "})\n",
    ".sort_values(\"roc_auc_cv\", ascending=False)\n",
    ".reset_index(drop=True))\n",
    "print(\"Сводка по ROC-AUC:\")\n",
    "print(summary.to_string(index=False))\n",
    "\n",
    "# лучшая модель по CV\n",
    "best_name, best_score = max(scores_cv_auc.items(), key=lambda kv: kv[1])\n",
    "best_cls_model = trained_cls[best_name]\n",
    "print(f\"\\n🏆 Лучшая модель: {best_name} → ROC-AUC (CV) = {best_score:.4f}\")\n",
    "\n",
    "elapsed_all = time.time() - start_all\n",
    "print(f\"\\n🏁 Общее время выполнения: {elapsed_all:.2f} секунд\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_df = pd.concat(all_results, ignore_index=True)\n",
    "top_results = all_results_df.sort_values(\"rank_test_score\").head(20)\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "top_results[[\"model\", \"mean_test_score\", \"rank_test_score\", \"params\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 1) выбираем ОБУЧЕННЫЙ пайплайн ====\n",
    "# если у тебя есть результаты грид-серча:\n",
    "# best_pipe = trained_cls[best_name]\n",
    "# иначе возьмём текущий pipe и при необходимости обучим:\n",
    "best_pipe = pipe\n",
    "\n",
    "# если препроцессор ещё не fit — обучим весь пайплайн на train\n",
    "try:\n",
    "    _ = best_pipe.named_steps[\"prep\"].transform(X_tr.iloc[:1].copy())\n",
    "except Exception:\n",
    "    best_pipe.fit(X_tr, y_tr)\n",
    "\n",
    "# ==== 2) helper: прогон до SelectKBest, если он есть ====\n",
    "def transform_up_to_select(fitted_pipeline, Xdf):\n",
    "    Xf = Xdf.copy()\n",
    "    # 2.1 feat-степ (если есть)\n",
    "    if \"feat\" in fitted_pipeline.named_steps:\n",
    "        Xf = fitted_pipeline.named_steps[\"feat\"].transform(Xf)\n",
    "    # 2.2 prep (обязательно уже fit)\n",
    "    prep = fitted_pipeline.named_steps[\"prep\"]\n",
    "    Xp = prep.transform(Xf)\n",
    "    feat_names = prep.get_feature_names_out()\n",
    "    # 2.3 select (опционально)\n",
    "    if \"select\" in fitted_pipeline.named_steps:\n",
    "        sel = fitted_pipeline.named_steps[\"select\"]\n",
    "        Xs = sel.transform(Xp)\n",
    "        feat_names = np.array(feat_names)[sel.get_support()]\n",
    "    else:\n",
    "        Xs = Xp\n",
    "    return Xs, np.array(feat_names)\n",
    "\n",
    "# ==== 3) получаем матрицы/имена для валидации и background ====\n",
    "X_val_sel, feat_names_sel = transform_up_to_select(best_pipe, X_val)\n",
    "\n",
    "rng = np.random.RandomState(RANDOM_STATE)\n",
    "bg_idx = rng.choice(len(X_tr), size=min(500, len(X_tr)), replace=False)\n",
    "X_bg_sel, _ = transform_up_to_select(best_pipe, X_tr.iloc[bg_idx])\n",
    "\n",
    "# ==== 4) SHAP для деревьев ====\n",
    "model = best_pipe.named_steps[\"clf\"]\n",
    "explainer = shap.TreeExplainer(model, data=X_bg_sel, feature_names=feat_names_sel)\n",
    "\n",
    "shap_values = explainer.shap_values(X_val_sel)\n",
    "sv = shap_values[1] if isinstance(shap_values, list) else shap_values\n",
    "\n",
    "# ==== 5) Таблица важностей mean|SHAP| ====\n",
    "mean_abs = np.abs(sv).mean(axis=0)\n",
    "shap_imp = (pd.DataFrame({\"feature\": feat_names_sel, \"mean_abs_shap\": mean_abs})\n",
    "            .sort_values(\"mean_abs_shap\", ascending=False))\n",
    "print(\"Top-20 features by mean|SHAP|:\")\n",
    "print(shap_imp.head(20).to_string(index=False))\n",
    "\n",
    "# ==== 6) Графики summary ====\n",
    "plt.figure(figsize=(8, 6))\n",
    "shap.summary_plot(sv, features=X_val_sel, feature_names=feat_names_sel,\n",
    "                  plot_type=\"bar\", show=False, max_display=20)\n",
    "plt.tight_layout(); plt.savefig(\"shap_summary_bar.png\", dpi=200); plt.show()\n",
    "\n",
    "shap.summary_plot(sv, features=X_val_sel, feature_names=feat_names_sel,\n",
    "                  plot_type=\"dot\", show=True, max_display=20)\n",
    "plt.tight_layout(); plt.savefig(\"shap_summary_beeswarm.png\", dpi=200)\n",
    "\n",
    "# (опционально) зависимости для ключевых фич\n",
    "for f in [\"bmi\", \"exercise_hours_per_week\", \"sedentary_hours_per_day\",\n",
    "          \"systolic_blood_pressure\", \"triglycerides\"]:\n",
    "    if f in feat_names_sel:\n",
    "        shap.dependence_plot(f, sv, X_val_sel, feature_names=feat_names_sel, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Наибольший вклад в предсказания даёт `diet` (питание).\n",
    "- Затем идут `bmi`, `triglycerides`, `systolic_blood_pressure`, `cholesterol`, `heart_rate`, `age`.\n",
    "- Такие признаки, как `gender`, `family_history`, `smoking`, `diabetes`, `alcohol_consumption` — почти не влияют (низкие SHAP-значения).\n",
    "\n",
    "Это совпадает с предыдущим feature importance, но SHAP ещё показывает направление и сильные взаимодействия.\n",
    "\n",
    "- `diet`: при низком значении (синий) риск возрастает.\n",
    "- `bmi` и `systolic_blood_pressure`: высокий уровень → повышает вероятность.\n",
    "- `exercise_hours_per_week`: больше спорта = меньший риск (сдвигает влево).\n",
    "- `BMI` ↔ `cholesterol`: видно нелинейность, риск растёт при очень высоком BMI и высоком холестерине.\n",
    "- `Exercise_hours_per_week` ↔ `activity_days`: регулярные тренировки действительно снижают риск.\n",
    "- `Sedentary_hours_per_day` ↔ `triglycerides`: больше сидения + высокие триглицериды → риск растёт.\n",
    "- `Systolic_blood_pressure` ↔ `age`: пожилой возраст + высокое давление → максимальный вклад в риск.\n",
    "- `Triglycerides` ↔ `sedentary_hours`: похожая история, комбинация усиливает эффект.\n",
    "\n",
    "Выводы:\n",
    "Модель в SHAP ведёт себя логично: питание, ИМТ, давление, триглицериды и спорт — ключевые факторы риска.\n",
    "Признаки “слабого влияния” (`gender`, `family_history`, `alcohol`, `smoking`) можно рассмотреть к удалению → упростит модель.\n",
    "`\n",
    "Есть сильные взаимодействия → стоит подумать о добавлении новых комбинаций признаков (например, `bmi` * `cholesterol`, `age` * `systolic_blood_pressure`).\n",
    "\n",
    "Отдельно тестировалось удаление не важных признаков и добавление новых, однако результатов это не дало. Поэтому сюда не включено это тестирование.\n",
    "\n",
    "Проблема переобучения остаётся (train ROC-AUC ≈ 1). Но SHAP подтверждает, что модель хотя бы не использует утечки (только осмысленные фичи)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Переобучение, подбор порога для F2 + SkBest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При построении итоговой модели на основе Random Forest мы стремились найти баланс между качеством классификации и устойчивостью к переобучению.\n",
    "Для этого параметры были выбраны следующим образом:\n",
    "- n_estimators=100 — достаточное количество деревьев, чтобы сгладить вариативность случайных выборок, но без избыточного роста времени обучения.\n",
    "- max_depth=8 — ограничение глубины деревьев снижает риск подгонки под шумные данные, что критично при низком качестве выборки.\n",
    "- min_samples_split=2 — позволяем деревьям учитывать даже небольшие ветвления, чтобы не потерять важные закономерности.\n",
    "- class_weight=\"balanced\" — компенсирует дисбаланс классов, увеличивая вклад миноритарного класса, что напрямую улучшает Recall и метрику F2.\n",
    "- random_state и n_jobs=-1 — фиксируем воспроизводимость результатов и параллелим вычисления для ускорения.\n",
    "\n",
    "Такая конфигурация позволяет контролировать переобучение: модель остаётся достаточно гибкой для захвата нелинейных связей, но не «запоминает» шум, что подтверждается разницей между метриками на train и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== данные ======\n",
    "use_cols = [c for c in heart_train.columns if c not in LEAK_COLS + [TARGET_COL]]\n",
    "X = heart_train[use_cols].copy()\n",
    "y = heart_train[TARGET_COL].astype(int).copy()\n",
    "\n",
    "# ====== пайплайн: препроцессор -> SelectKBest -> RandomForest (твои лучшие параметры) ======\n",
    "rf_best = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=8,\n",
    "    min_samples_split=2,\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"prep\",   preprocessor),                  # твой ColumnTransformer\n",
    "    (\"select\", SelectKBest(score_func=SCORE_FUNC, k=K_BEST)),\n",
    "    (\"clf\",    rf_best),\n",
    "])\n",
    "\n",
    "# ====== hold-out валидация ======\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# обучаем на train\n",
    "pipe.fit(X_tr, y_tr)\n",
    "\n",
    "# вероятности на валидации\n",
    "val_proba = pipe.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# — подбор порога по F2 на валидации —\n",
    "thresholds = np.linspace(0.01, 0.99, 99)\n",
    "f2_vals = [fbeta_score(y_val, (val_proba >= t).astype(int), beta=2) for t in thresholds]\n",
    "t_best = float(thresholds[int(np.argmax(f2_vals))])\n",
    "\n",
    "# метрики на валидации при лучшем пороге\n",
    "val_pred = (val_proba >= t_best).astype(int)\n",
    "val_metrics = {\n",
    "    \"F2\":       fbeta_score(y_val, val_pred, beta=2),\n",
    "    \"F1\":       f1_score(y_val, val_pred),\n",
    "    \"Recall\":   recall_score(y_val, val_pred),\n",
    "    \"Precision\":precision_score(y_val, val_pred),\n",
    "    \"ROC-AUC\":  roc_auc_score(y_val, val_proba),\n",
    "    \"PR-AUC\":   average_precision_score(y_val, val_proba),\n",
    "    \"BestThreshold\": t_best\n",
    "}\n",
    "print(\"📊 Random Forest - @best F2-threshold:\", {k: round(v, 4) if isinstance(v, float) else v for k, v in val_metrics.items()})\n",
    "\n",
    "# — метрики на TRAIN при том же пороге (t_best), без подгонки под train —\n",
    "train_proba = pipe.predict_proba(X_tr)[:, 1]\n",
    "train_pred  = (train_proba >= t_best).astype(int)\n",
    "\n",
    "train_metrics = {\n",
    "    \"F2\":        fbeta_score(y_tr, train_pred, beta=2),\n",
    "    \"F1\":        f1_score(y_tr, train_pred),\n",
    "    \"Recall\":    recall_score(y_tr, train_pred),\n",
    "    \"Precision\": precision_score(y_tr, train_pred),\n",
    "    \"ROC-AUC\":   roc_auc_score(y_tr, train_proba),\n",
    "    \"PR-AUC\":    average_precision_score(y_tr, train_proba),\n",
    "    \"Threshold\": t_best\n",
    "}\n",
    "print(\"📊 Random Forest - TRAIN @val F2-threshold:\",\n",
    "      {k: round(v, 4) if isinstance(v, float) else v for k, v in train_metrics.items()})\n",
    "\n",
    "# какие фичи выбрал SelectKBest\n",
    "feat_names = pipe.named_steps[\"prep\"].get_feature_names_out()\n",
    "mask = pipe.named_steps[\"select\"].get_support()\n",
    "selected_features = np.array(feat_names)[mask]\n",
    "print(f\"Выбрано признаков SelectKBest: {mask.sum()} из {len(feat_names)}\")\n",
    "for f in selected_features: print(\" -\", f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На грязных и шумных данных ROC-AUC почти всегда страдает. Но даже если модель переобучается (Train = 1.0, Val ≈ 0.6), на валидации всё равно можно держать стабильный F2 > 0.7 — что уже успех для такой задачи.\n",
    "\n",
    "Результаты и выводы\n",
    "- На валидации достигнуты следующие показатели при оптимальном пороге 0.39:\n",
    "- F2 = 0.734\n",
    "- Recall = 1.0\n",
    "- Precision ≈ 0.355\n",
    "- ROC-AUC = 0.558, PR-AUC = 0.416\n",
    "\n",
    "На обучении метрики заметно выше:\n",
    "- F2 = 0.739,\n",
    "- ROC-AUC ≈ 0.919,\n",
    "- PR-AUC ≈ 0.865\n",
    "\n",
    "Это указывает на ожидаемую разницу: модель хорошо подстраивается под train, но валидационные ROC-AUC и PR-AUC заметно ниже, что отражает слабое качество исходных данных и наличие шума.\n",
    "\n",
    "Главное достижение — максимальный Recall (1.0), что соответствует цели проекта: минимизировать пропуск положительных случаев. Метрика F2 > 0.73 подтверждает, что модель делает ставку на полноту за счёт точности, что оправдано бизнес-требованием.\n",
    "\n",
    "Так же мы добились того, что модель перестала переобучаться полностью на обучающей выборке (ранее ROC-AUC на трейне был близок к 1.0, что указывало на запоминание данных).\n",
    "После настройки параметров глубины (max_depth=8) и ограничения структуры деревьев, ROC-AUC на трейне снизился до 0.9188.\n",
    "Это означает, что модель стала более реалистично оценивать сложность задачи, а её качество на трейне и валидации теперь разделяется меньше искусственным образом. Такой шаг можно считать успехом в борьбе с переобучением: модель лучше отражает закономерности данных, а не шум."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Предсказание на тесте"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делаем предсказания на тесте и сохраняет в отдельный файл .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== предсказания на heart_test (id в индексе) ======\n",
    "X_test = heart_test[[c for c in use_cols if c in heart_test.columns]].copy()\n",
    "\n",
    "# дообучаем пайплайн на всём train перед инференсом\n",
    "pipe.fit(X, y)\n",
    "\n",
    "# вероятности и метки с учётом порога\n",
    "test_proba = pipe.predict_proba(X_test)[:, 1]\n",
    "test_pred  = (test_proba >= t_best).astype(int)\n",
    "\n",
    "# сабмишн: две колонки — id и prediction\n",
    "sub = pd.DataFrame({\"id\": heart_test.index, \"prediction\": test_pred})\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Файл submission.csv сохранён:\", sub.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
